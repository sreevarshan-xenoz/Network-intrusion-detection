name: Automated Model Retraining

on:
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force retraining even if performance is good'
        required: false
        default: 'false'
        type: boolean
      dataset_path:
        description: 'Path to new training dataset'
        required: false
        type: string

env:
  PYTHON_VERSION: '3.10'

jobs:
  check-model-performance:
    name: Check Current Model Performance
    runs-on: ubuntu-latest
    outputs:
      needs_retraining: ${{ steps.check.outputs.needs_retraining }}
      current_accuracy: ${{ steps.check.outputs.current_accuracy }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Check model performance
      id: check
      run: |
        python -c "
        import sys
        from src.models.evaluator import ModelEvaluator
        from src.models.registry import ModelRegistry
        
        registry = ModelRegistry()
        evaluator = ModelEvaluator()
        
        # Get current model performance
        current_model = registry.get_active_model()
        if not current_model:
            print('needs_retraining=true')
            print('current_accuracy=0.0')
            sys.exit(0)
        
        # Evaluate current model on recent data
        accuracy = evaluator.evaluate_model_performance(current_model)
        
        # Check if retraining is needed
        needs_retraining = accuracy < 0.90 or '${{ inputs.force_retrain }}' == 'true'
        
        print(f'needs_retraining={str(needs_retraining).lower()}')
        print(f'current_accuracy={accuracy}')
        " >> $GITHUB_OUTPUT

  retrain-models:
    name: Retrain ML Models
    runs-on: ubuntu-latest
    needs: check-model-performance
    if: needs.check-model-performance.outputs.needs_retraining == 'true'
    
    services:
      mongodb:
        image: mongo:6
        ports:
          - 27017:27017
        env:
          MONGO_INITDB_ROOT_USERNAME: admin
          MONGO_INITDB_ROOT_PASSWORD: password
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Create directories
      run: |
        mkdir -p data/datasets data/processed data/models logs

    - name: Download training datasets
      run: |
        # Download latest datasets
        echo "Downloading training datasets..."
        # Add commands to download datasets from secure storage
        # wget -O data/datasets/nsl-kdd.zip https://secure-storage/nsl-kdd.zip
        # wget -O data/datasets/cicids.zip https://secure-storage/cicids.zip

    - name: Prepare training data
      run: |
        python -c "
        from src.data.loaders import NSLKDDLoader, CICIDSLoader
        from src.data.preprocessing.feature_encoder import FeatureEncoder
        from src.data.preprocessing.feature_scaler import FeatureScaler
        from src.data.preprocessing.class_balancer import ClassBalancer
        
        print('Preparing training data...')
        # Load and preprocess data
        # Implementation would go here
        print('Data preparation completed')
        "

    - name: Train models
      env:
        MONGODB_URL: mongodb://admin:password@localhost:27017/nids?authSource=admin
      run: |
        python -c "
        from src.models.trainer import ModelTrainer
        
        trainer = ModelTrainer()
        print('Starting model training...')
        
        # Train all models
        results = trainer.train_all_models()
        
        print('Model training completed')
        print(f'Training results: {results}')
        "

    - name: Evaluate new models
      run: |
        python -c "
        from src.models.evaluator import ModelEvaluator
        from src.models.registry import ModelRegistry
        
        evaluator = ModelEvaluator()
        registry = ModelRegistry()
        
        # Get latest trained models
        models = registry.list_models(limit=5)
        
        for model in models:
            metrics = evaluator.evaluate_model(model)
            print(f'Model {model.model_id}: Accuracy={metrics.get(\"accuracy\", 0):.3f}')
        "

    - name: Select best model
      id: select
      run: |
        python -c "
        from src.models.registry import ModelRegistry
        from src.models.evaluator import ModelEvaluator
        
        registry = ModelRegistry()
        evaluator = ModelEvaluator()
        
        # Select best performing model
        best_model = evaluator.select_best_model()
        
        if best_model:
            print(f'best_model_id={best_model.model_id}')
            print(f'best_model_accuracy={best_model.performance_metrics.get(\"accuracy\", 0)}')
        else:
            print('best_model_id=none')
            print('best_model_accuracy=0.0')
        " >> $GITHUB_OUTPUT

    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      with:
        name: trained-models
        path: |
          data/models/
          logs/training.log

  validate-new-model:
    name: Validate New Model
    runs-on: ubuntu-latest
    needs: retrain-models
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        name: trained-models

    - name: Validate model performance
      run: |
        python -c "
        from src.models.evaluator import ModelEvaluator
        
        evaluator = ModelEvaluator()
        
        # Run comprehensive validation
        validation_results = evaluator.validate_model_deployment()
        
        print('Model validation results:')
        for metric, value in validation_results.items():
            print(f'{metric}: {value}')
        
        # Check if model meets deployment criteria
        accuracy = validation_results.get('accuracy', 0)
        if accuracy < 0.85:
            raise Exception(f'Model accuracy {accuracy} below deployment threshold')
        "

    - name: Run A/B testing simulation
      run: |
        python -c "
        from src.models.evaluator import ModelEvaluator
        
        evaluator = ModelEvaluator()
        
        # Simulate A/B testing between old and new model
        ab_results = evaluator.run_ab_test_simulation()
        
        print('A/B testing results:')
        print(f'New model improvement: {ab_results.get(\"improvement\", 0):.2%}')
        
        if ab_results.get('improvement', 0) < 0.02:  # Less than 2% improvement
            print('WARNING: New model shows minimal improvement')
        "

  deploy-new-model:
    name: Deploy New Model
    runs-on: ubuntu-latest
    needs: [retrain-models, validate-new-model]
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download model artifacts
      uses: actions/download-artifact@v3
      with:
        name: trained-models

    - name: Deploy to model registry
      run: |
        echo "Deploying new model to production registry..."
        # Add deployment commands here
        # This could involve:
        # - Uploading to model registry
        # - Updating model serving configuration
        # - Rolling deployment with canary testing

    - name: Update API service
      run: |
        echo "Updating API service with new model..."
        # Trigger API service update to load new model

    - name: Run post-deployment validation
      run: |
        # Wait for deployment to complete
        sleep 60
        
        # Run health checks
        python scripts/health_check.py --verbose
        
        # Validate API is serving predictions with new model
        python -c "
        import requests
        import json
        
        # Test API with sample data
        response = requests.post('http://api:8000/predict', 
                               json={'sample': 'test_data'})
        
        if response.status_code == 200:
            result = response.json()
            print(f'API test successful: {result}')
        else:
            raise Exception(f'API test failed: {response.status_code}')
        "

  notify-completion:
    name: Notify Retraining Completion
    runs-on: ubuntu-latest
    needs: [check-model-performance, retrain-models, validate-new-model, deploy-new-model]
    if: always()
    
    steps:
    - name: Send notification
      run: |
        echo "Model retraining workflow completed"
        echo "Status: ${{ job.status }}"
        echo "Previous accuracy: ${{ needs.check-model-performance.outputs.current_accuracy }}"
        echo "New model ID: ${{ needs.retrain-models.outputs.best_model_id }}"
        
        # Add notification logic here (Slack, email, etc.)
        # curl -X POST -H 'Content-type: application/json' \
        #   --data '{"text":"Model retraining completed with status: ${{ job.status }}"}' \
        #   ${{ secrets.SLACK_WEBHOOK_URL }}